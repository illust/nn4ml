讲义2a

几种主要类型神经网络的概述：
1）前馈神经网络(Feed-forward neural networks)
这是在实际应用中最常见的神经网络类型
——第一层是输入端，最后一层是输出端
——如果网络中存在超过一层的隐含层，我们叫它“深度”神经网络
它们计算出一系列层次之间改变相似性的变换
——每一层的激活神经元是下面一层活动的非线性函数

2） 循环网络(Recurrent networks)
它们的连接图中具有定向循环
——这意味着有时候你可以从你开始的地方跟随箭头返回到起始点
它们具有非常复杂的动态性，这使得它们很难被训练
——目前有很多有趣的研究用于发现高效的训练循环网络的方法
它们更具有生物现实性

循环神经网络用于序列建模
循环神经网络是一种非常自然的序列数据的建模方法：
——每个时间片段它们相当于带有一级隐含层的极深网络
——除此之外它们在每个时间片使用相同的权重，并且它们在每个时间片获得输入。
它们具有在它们的隐含阶段长时期记忆信息的能力
——但是很难训练它们使用这种能力

一个用来解释当前RNN能够用来做什么的例子(激发你的兴趣！)
Ilya Sutskever(2011)训练了一种特殊类型的RNN，用来预测序列中的下一个字符
在对英文维基五十亿字符的长时期训练之后，他能够成功产生一段新文本
——它通过预测下一个字符的概率分布，然后从这个分布中抽样一个字符来生成字符


3) 对称连接网络(Symmetrically connected networks)
这类网络与循环网络相似，但是它们单元之间的连接是对称的(它们在两个方向上具有相同>的权值)
——John Hopfield(和其他人)意识到对称网络比循环网络更容易分析
——它们也同时在功能上有所限制。因为它们遵守了能量函数，比如，它们不能进行循环建模
没有隐含单元的对称连接网络也称之为"Hopfield网络"

带有隐含层的对称连接网络
它们被称之为"玻尔兹曼机"(Boltzmann machines)
——它们是一种比Hopfield网络更加强大的模型
——它们是一种比RNN能力弱小一些的网络
——它们具有简洁美观的学习算法
我们将在课程的末期学习到玻尔兹曼机

讲义2b
感知机：神经网络的第一代
统计模式识别的标准范例
1.将原始输入向量转换为特征激活的向量
使用基于常识的手写程序来定义特征
2.学习如何权量每个功能激活以获取单个标量数量
3.如果这个数量高于某个阈值，就确定输入向量是目标类的正例

感知机的历史
它们是20世纪60年代早期通过Frank Rosenblatt流行起来的
——它们似乎有一种非常强大的学习算法
——许多伟大的提议因它们所能学习去完成的而产生
在1969年，Minsky和Papert发表了一本名为“感知机”的书，分析它们的功能并展现出它们的
限制
——很多人思考这些限制可以应用到所有的神经网络模型
感知机学习过程在今天仍然广泛使用，用于解决那些包含数百万特征的巨大特征向量的任务

二值门限神经元(决策单元)
McCulloch-Pitts(1943)
——首次计算出其他神经元输入的权值之和(再加一个偏置)
——如果权值之和超过0则输出为1

如何使用与我们用来学习权值的相同规则去学习偏置
一个阈值等价于有一个负偏置
我们能使用一个技巧避免必须找出单独的偏置学习规则
——一个偏置完全等价于激活值总为1的一个额外输入流的权值
——现在如果偏置是一个权值的话我们就可以学习它了

感知机收敛过程：训练二值输出神经元作为分类器
增加一个额外的值为1的分量到每个输入向量。此分量上的“偏置”权值减去阈值。现在，我>们可以忘掉这个阈值了
使用任何方法选择训练案例，确保每个训练案例将继续被挑选
——如果输出单元是正确的，单独保留它的权值
——如果输出单元不正确输出为0,把输入向量增加到权值向量
——如果输出单元不正确输出为1,从权值向量中减去输入向量
这保证找到一组权重，如果任何这样的集合存在，所有的训练案例获得正确的答案


讲义2c
感知机的几何意义
警告！！
对于非数学专业的人来说，这部分将比之前讲义的内容更加复杂
——你可能必须花费很长一段时间学习下面两部分
如果你不习惯在高维空间思考超平面，现在是学习的时候
为了在14维空间处理超平面，可视化一个3维空间然后大声对自己喊“这是14维”。23333333
每个人都是这样做的
但是记住从13维到14维比2维到3维创造出额外复杂性

权值空间
这个空间中每一个权值具有一维
空间中的一个点代表所有权值的特定设置
假设我们已经消除了阈值，每个训练案例可以表示为通过原点的超平面
——权值必须位于超平面的一边以获得正确的答案

每一个训练案例定义一个平面(用黑线表示)
——平面通过原点并垂直于输入向量

